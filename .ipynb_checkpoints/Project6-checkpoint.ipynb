{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.style.use('whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "matplotlib.rcParams.update({'figure.figsize': (10, 6)})\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'axes.labelsize': 20})\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'font.family': 'Helvetica, Arial, sans-serif'})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "      <td>5.394000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.149807e-16</td>\n",
       "      <td>2.508108e-16</td>\n",
       "      <td>-2.107654e-17</td>\n",
       "      <td>-2.023348e-16</td>\n",
       "      <td>-4.002434e-15</td>\n",
       "      <td>1.175017e-16</td>\n",
       "      <td>-1.095980e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.261446e+00</td>\n",
       "      <td>-5.109073e+00</td>\n",
       "      <td>-5.020884e+00</td>\n",
       "      <td>-5.014510e+00</td>\n",
       "      <td>-1.308748e+01</td>\n",
       "      <td>-6.470013e+00</td>\n",
       "      <td>-9.040868e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.395154e-01</td>\n",
       "      <td>-9.103164e-01</td>\n",
       "      <td>-8.882717e-01</td>\n",
       "      <td>-8.909378e-01</td>\n",
       "      <td>-5.231005e-01</td>\n",
       "      <td>-6.521325e-01</td>\n",
       "      <td>-7.476738e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.066190e-01</td>\n",
       "      <td>-2.777527e-02</td>\n",
       "      <td>-2.147379e-02</td>\n",
       "      <td>-1.237607e-02</td>\n",
       "      <td>3.531645e-02</td>\n",
       "      <td>-2.046032e-01</td>\n",
       "      <td>-3.839636e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.106635e-01</td>\n",
       "      <td>7.210475e-01</td>\n",
       "      <td>7.052356e-01</td>\n",
       "      <td>7.103118e-01</td>\n",
       "      <td>5.239313e-01</td>\n",
       "      <td>6.904554e-01</td>\n",
       "      <td>3.487834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.885992e+00</td>\n",
       "      <td>4.465161e+00</td>\n",
       "      <td>4.654922e+01</td>\n",
       "      <td>4.004720e+01</td>\n",
       "      <td>1.204128e+01</td>\n",
       "      <td>1.680151e+01</td>\n",
       "      <td>3.732404e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat             x             y             z         depth  \\\n",
       "count  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04   \n",
       "mean   2.149807e-16  2.508108e-16 -2.107654e-17 -2.023348e-16 -4.002434e-15   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.261446e+00 -5.109073e+00 -5.020884e+00 -5.014510e+00 -1.308748e+01   \n",
       "25%   -8.395154e-01 -9.103164e-01 -8.882717e-01 -8.909378e-01 -5.231005e-01   \n",
       "50%   -2.066190e-01 -2.777527e-02 -2.147379e-02 -1.237607e-02  3.531645e-02   \n",
       "75%    5.106635e-01  7.210475e-01  7.052356e-01  7.103118e-01  5.239313e-01   \n",
       "max    8.885992e+00  4.465161e+00  4.654922e+01  4.004720e+01  1.204128e+01   \n",
       "\n",
       "              table         price  \n",
       "count  5.394000e+04  5.394000e+04  \n",
       "mean   1.175017e-16 -1.095980e-16  \n",
       "std    1.000000e+00  1.000000e+00  \n",
       "min   -6.470013e+00 -9.040868e-01  \n",
       "25%   -6.521325e-01 -7.476738e-01  \n",
       "50%   -2.046032e-01 -3.839636e-01  \n",
       "75%    6.904554e-01  3.487834e-01  \n",
       "max    1.680151e+01  3.732404e+00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/shivam2503/diamonds/data\n",
    "diamonds = pd.read_csv('data/diamonds.csv')\n",
    "\n",
    "from copy import deepcopy\n",
    "'''\n",
    "data = deepcopy(diamonds[['carat', 'x', 'y', 'z', 'depth', 'table',\n",
    "             'clarity', 'color']])\n",
    "'''\n",
    "data = deepcopy(diamonds[['carat', 'x', 'y', 'z', 'depth', 'table', 'price',\n",
    "                          'clarity', 'color']])\n",
    "target = deepcopy(diamonds['cut']).astype('category')\n",
    "\n",
    "for col in ['carat','x','y','z','depth','table','price']:\n",
    "    data[col] = (data[col] - data[col].mean()) / data[col].std()\n",
    "\n",
    "\n",
    "for col in ['color', 'clarity']:\n",
    "    data[col] = data[col].astype('category');\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "##### KERAS #####\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85476586  0.81030203  0.88035601  0.38432704  0.69045541]\n",
      " [ 0.72104751  0.76652436  0.55443795 -1.36072587  1.58551401]\n",
      " [-0.42893033 -0.41547286 -0.45165694 -0.38349624 -0.6521325 ]\n",
      " ..., \n",
      " [ 0.9082532   0.93287952  1.02205952  0.80313973 -0.20460319]\n",
      " [ 0.15051586  0.1011037   0.21434954  0.73333762 -0.20460319]\n",
      " [ 0.40903801  0.46883617  0.52609725  0.73333762 -0.6521325 ]]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "numeric_headers = ['carat', 'x', 'y', 'z', 'depth', 'table']\n",
    "\n",
    "X_train_num =  data_train[numeric_headers].values\n",
    "X_test_num = data_test[numeric_headers].values\n",
    "\n",
    "print(X_train_num)\n",
    "print(X_train_num.shape[1])\n",
    "\n",
    "encoders = dict() \n",
    "categorical_headers = ['color','clarity']\n",
    "\n",
    "for col in categorical_headers:\n",
    "    data_train[col] = data_train[col].str.strip()\n",
    "    data_test[col] = data_test[col].str.strip()\n",
    "    encoders[col] = LabelEncoder()\n",
    "    data_train[col+'_int'] = encoders[col].fit_transform(data_train[col])\n",
    "    data_test[col+'_int'] = encoders[col].transform(data_test[col])\n",
    "    \n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input layers Tensor(\"numeric_data_8:0\", shape=(?, 5), dtype=float32)\n",
      "13873      Premium\n",
      "16986    Very Good\n",
      "44548        Ideal\n",
      "49166         Good\n",
      "45610        Ideal\n",
      "Name: cut, dtype: category\n",
      "Categories (5, object): [Fair, Good, Ideal, Premium, Very Good]\n",
      "[array([17, 35, 38, ..., 39, 21, 18]), array([3, 0, 3, ..., 4, 0, 4]), array([2, 5, 5, ..., 5, 3, 2])]\n",
      "[[ 0.85476586  0.81030203  0.88035601  0.38432704  0.69045541]\n",
      " [ 0.72104751  0.76652436  0.55443795 -1.36072587  1.58551401]\n",
      " [-0.42893033 -0.41547286 -0.45165694 -0.38349624 -0.6521325 ]\n",
      " ..., \n",
      " [ 0.9082532   0.93287952  1.02205952  0.80313973 -0.20460319]\n",
      " [ 0.15051586  0.1011037   0.21434954  0.73333762 -0.20460319]\n",
      " [ 0.40903801  0.46883617  0.52609725  0.73333762 -0.6521325 ]]\n",
      "Epoch 1/5\n",
      "43152/43152 [==============================] - 6s - loss: 0.8645 - categorical_accuracy: 0.6668     \n",
      "Epoch 2/5\n",
      "43152/43152 [==============================] - 5s - loss: 0.7683 - categorical_accuracy: 0.6979     \n",
      "Epoch 3/5\n",
      "43152/43152 [==============================] - 5s - loss: 0.7577 - categorical_accuracy: 0.6952     \n",
      "Epoch 4/5\n",
      "43152/43152 [==============================] - 5s - loss: 0.7490 - categorical_accuracy: 0.6976     \n",
      "Epoch 5/5\n",
      "43152/43152 [==============================] - 5s - loss: 0.7412 - categorical_accuracy: 0.6994     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f49f080>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment key\n",
    "# #   = My comments\n",
    "# ##  = Larson comments\n",
    "# ### = Changed code\n",
    "\n",
    "\n",
    "# Columns to generate 1-hots\n",
    "#  education    X occupation\n",
    "#  country      X occupation\n",
    "#  relationship X marital_status X sex\n",
    "#  race         X sex\n",
    "cross_columns = [['clarity', 'color']]\n",
    "\n",
    "\n",
    "# Define the order and layers of the network????\n",
    "## we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "\n",
    "# For all sets of columns to be crossed\n",
    "for cols in cross_columns:\n",
    "    # Creates labels for categorical data. And stores the map\n",
    "    #  enc.transform translates categorical data to integer\n",
    "    ## encode crossed columns as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    ## create crossed labels\n",
    "    # Create dataframe of the columns cross product as string data\n",
    "    X_crossed_train = data_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = data_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # fits the label encoder to the [x_crossed_train, x_crossed_test]\n",
    "    enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "    # transform the string to integer for train data\n",
    "    X_crossed_train = enc.transform(X_crossed_train)\n",
    "    # transform the string to integer for test data\n",
    "    X_crossed_test = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # Add elements of x_crossed_train to list of previous x_crossed_train values\n",
    "    X_ints_train.append( X_crossed_train )\n",
    "    # Same with test data\n",
    "    X_ints_test.append( X_crossed_test )\n",
    "    \n",
    "    ## get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) ## same as the max(df_train[col])\n",
    "    ## create embedding branch from the number of categories\n",
    "    # Create inputs for each of the crossed columns\n",
    "    inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "    # Adds the Inputs to a list\n",
    "    all_inputs.append(inputs)\n",
    "    # Creates an Embedding with input number of categories and output the sqrt(#categories). \n",
    "    #  input_length is max matrix size of input\n",
    "    # Passes inputs into embedding\n",
    "\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    \n",
    "    # Flatten the dimension of the Embedding\n",
    "    x = Flatten()(x)\n",
    "    # Add the Flattened Embedding to the list\n",
    "    all_branch_outputs.append(x)\n",
    "\n",
    "# Merge all of the Flattened branches to create a wide_branch\n",
    "## merge the branches together\n",
    "### wide_branch = concatenate(all_branch_outputs)\n",
    "wide_branch = all_branch_outputs[0]\n",
    "\n",
    "## reset this input branch\n",
    "all_branch_outputs = []\n",
    "## add in the embeddings\n",
    "for col in categorical_headers_ints:\n",
    "    ## encode as ints for the embedding\n",
    "    X_ints_train.append( data_train[col].values ) # append NDarrays to list\n",
    "    X_ints_test.append( data_test[col].values )\n",
    "    \n",
    "    ## get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) ## same as the max(df_train[col])\n",
    "    # Input defines the tensor shape\n",
    "    ## create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    # Create a list of all the Inputs\n",
    "    all_inputs.append(inputs)\n",
    "    \n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "## also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False, name='numeric_data'))\n",
    "# Create 20 node dense NN with relu activation\n",
    "print(\"input layers\", all_inputs[-1])\n",
    "'''\n",
    "x = Dense(units=5, activation='relu')(all_inputs[-1])\n",
    "'''\n",
    "x = Dense(units=5, activation='relu')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "## merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "'''\n",
    "deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "'''\n",
    "deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "    \n",
    "final_branch = concatenate([wide_branch, deep_branch])\n",
    "final_branch = Dense(units=5,activation='sigmoid')(final_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "'''model.compile(optimizer='Nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy', 'accuracy'])\n",
    "              \n",
    "    SGD = 41%\n",
    "    RMSprop(lr=0.001) = 57%\n",
    "    Adagrad = 55%\n",
    "    Adadelta = 55%\n",
    "    Nadam(epochs=20) = 70%\n",
    "'''\n",
    "# Best is categorical_crossentropy and mean_squared_error\n",
    "model.compile(optimizer='Nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "print(target_train.head())\n",
    "\n",
    "print(X_ints_train)\n",
    "print(X_train_num\n",
    "     )\n",
    "\n",
    "target_train_one_hot = pd.get_dummies(target_train)\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        np.asarray(target_train_one_hot), epochs=5, batch_size=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_ints_test+[X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 304   14    6   14    3]\n",
      " [  81  525   19   64  268]\n",
      " [   5    4 3864  199  240]\n",
      " [   0    1  293 2272  194]\n",
      " [   7  225  507  487 1192]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11aada9b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABEUAAALHCAYAAABos/xRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aac8358>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 355,
       "width": 546
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "target_test_one_hot = pd.get_dummies(target_test)\n",
    "cnf = confusion_matrix(\n",
    "    np.asarray(target_test_one_hot).argmax(1),\n",
    "    y_hat.argmax(1)\n",
    ")\n",
    "print(cnf)\n",
    "cnf_norm = cnf.astype('float') / cnf.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cnf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "class WideDeepNetwork():\n",
    "    def __init__(\n",
    "        self, epochs=5, batch_size=35, activation='relu', final_activation='sigmoid', optimizer='adagrad', \n",
    "        loss='mean_squared_error', metrics=['accuracy'], deep_input_size=5, \n",
    "        deep_layer_sizes=[50,10], numeric_features=None, categorical_features=None,\n",
    "        cross_categories=None,\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.final_activation = final_activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.deep_input_size = deep_input_size\n",
    "        self.deep_layer_sizes = deep_layer_sizes\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.cross_categories = cross_categories\n",
    "        \n",
    "    def _make_wide_network(self):\n",
    "        ## we need to create separate sequential models for each embedding\n",
    "        embed_branches = []\n",
    "        X_ints_train = []\n",
    "        all_inputs = []\n",
    "        all_branch_outputs = []\n",
    "        \n",
    "        if not all(isinstance(element, list) for element in self.cross_categories):\n",
    "            raise ValueError('cross_categories should be type [[]]')\n",
    "\n",
    "        # For all sets of columns to be crossed\n",
    "        for cols in self.cross_categories:\n",
    "            # Gets number of 1-hot encoded crossed classes given categories and values\n",
    "            N = 1\n",
    "            for col in cols:\n",
    "                N *= len(self.categorical_features[col])\n",
    "\n",
    "            ## create embedding branch from the number of categories\n",
    "            # Create inputs for each of the crossed columns\n",
    "            inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "            # Adds the Inputs to a list\n",
    "            all_inputs.append(inputs)\n",
    "            # Creates an Embedding with input number of categories and output the sqrt(#categories). \n",
    "            #  input_length is max matrix size of input\n",
    "            # Passes inputs into embedding\n",
    "            x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "            # Flatten the dimension of the Embedding\n",
    "            x = Flatten()(x)\n",
    "            # Add the Flattened Embedding to the list\n",
    "            all_branch_outputs.append(x)\n",
    "\n",
    "        # Merge all of the Flattened branches to create a wide_branch\n",
    "        ## merge the branches together\n",
    "        ### wide_branch = concatenate(all_branch_outputs)\n",
    "        if(len(all_branch_outputs) == 1):\n",
    "            wide_branch = all_branch_outputs[0]\n",
    "        else:\n",
    "            wide_branch = concatinate(all_branch_outputs)\n",
    "        return all_inputs, wide_branch\n",
    "    \n",
    "    def _make_deep_network(self):\n",
    "        \n",
    "        if not all(isinstance(element, list) for _, element in self.categorical_features.items()):\n",
    "            raise ValueError('Categorical features must be a dictonary of lists')\n",
    "        \n",
    "        all_inputs = []  \n",
    "        all_branch_outputs = []\n",
    "        ## add in the embeddings\n",
    "        for key, l in self.categorical_features.items():\n",
    "            # Get highest int classification of category feature\n",
    "            N = len(l)-1\n",
    "\n",
    "            # Input defines the tensor shape\n",
    "            ## create embedding branch from the number of categories\n",
    "            inputs = Input(shape=(1,),dtype='int32', name=key)\n",
    "            # Create a list of all the Inputs\n",
    "            all_inputs.append(inputs)\n",
    "\n",
    "            x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "\n",
    "            x = Flatten()(x)\n",
    "            all_branch_outputs.append(x)\n",
    "\n",
    "        ## also get a dense branch of the numeric features\n",
    "        if len(self.numeric_features) < 1:\n",
    "            raise ValueError('Numeric Features must not be empty')\n",
    "            \n",
    "        all_inputs.append(Input(shape=(len(self.numeric_features),),sparse=False, name='numeric_data'))\n",
    "\n",
    "        # Gets a dense encoding of all the numerical data\n",
    "        x = Dense(units=self.deep_input_size, activation=self.activation)(all_inputs[-1])\n",
    "        all_branch_outputs.append( x )\n",
    "\n",
    "        # merge the branches together\n",
    "        deep_branch = concatenate(all_branch_outputs)\n",
    "        \n",
    "        for units in self.deep_layer_sizes:\n",
    "            deep_branch = Dense(units=units,activation=self.activation)(deep_branch)\n",
    "            \n",
    "        return all_inputs, deep_branch\n",
    "        \n",
    "    def make_model(self):\n",
    "        wide_input, wide_branch = self._make_wide_network()\n",
    "        deep_input, deep_branch = self._make_deep_network()\n",
    "        all_inputs = wide_input + deep_input\n",
    "        \n",
    "        final_branch = concatenate([wide_branch, deep_branch])\n",
    "        final_branch = Dense(units=5,activation=self.final_activation)(final_branch)\n",
    "        \n",
    "        self.model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "        \n",
    "        self.model.compile(optimizer='Nadam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['categorical_accuracy'])\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        X_train = []\n",
    "        # Get crossed categorical values\n",
    "        for cols in self.cross_categories:\n",
    "            X_train.append(X[cols].apply(lambda x: '_'.join(x), axis=1))\n",
    "        for col in list(self.categorical_features.keys()):\n",
    "            X_train.append(X_catagorical[col].cat.codes)    \n",
    "        X_numerical = X[self.numeric_features]\n",
    "        \n",
    "        X_train = np.hstack((np.asarray(X_catagorical), np.asarray(X_numerical)))\n",
    "        y_one_hot = np.asarray(pd.get_dummies(target_train))\n",
    "        self.model.fit(X_train, y_one_hot, \n",
    "                       epochs=self.epochs, batch_size=self.batch_size, \n",
    "                       verbose=print_progress)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_hat = predict(X)\n",
    "        return y_hat\n",
    "        \n",
    "    # [CITE] http://algoadventures.com/sklearn-from-the-source-code-up-basics/\n",
    "    # ClassifierMixin implementation\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
    "    # end implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y', 'z', 'depth', 'table']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model expects 4 input arrays, but only received one array. Found: array with shape (53940, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-c56524adb9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwdNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mwdNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-116b55eb5353>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    129\u001b[0m         self.model.fit(X_train, y_one_hot, \n\u001b[1;32m    130\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                        verbose=print_progress)\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    109\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                              \u001b[0;34m' arrays, but only received one array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                              'Found: array with shape ' + str(data.shape))\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model expects 4 input arrays, but only received one array. Found: array with shape (53940, 7)"
     ]
    }
   ],
   "source": [
    "numeric_featues = numeric_headers\n",
    "\n",
    "print(numeric_featues)\n",
    "cross_categories = [['clarity','color']]\n",
    "categorical_features = {'clarity': data['clarity'].value_counts().keys().tolist(), \n",
    "                        'color': data['color'].value_counts().keys().tolist()}\n",
    "\n",
    "\n",
    "wdNet = WideDeepNetwork(numeric_features=numeric_featues, \n",
    "                        categorical_features=categorical_features, \n",
    "                        cross_categories=cross_categories)\n",
    "\n",
    "wdNet.make_model()\n",
    "wdNet.fit(data, target, print_progress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-776cc7d6d7c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# ROC Area under Curve score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m roc_auc_score(\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my_hat_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# gets the column of the maximum for each row\n",
    "# then converts it to binary (manual one-hot encoding)\n",
    "y_hat_one_hot = np.zeros(y_hat.shape)\n",
    "y_hat_one_hot[np.arange(y_hat.shape[0]), y_hat.argmax(1)] = 1\n",
    "\n",
    "# ROC Area under Curve score\n",
    "roc_auc_score(\n",
    "    np.asarray(y_test),\n",
    "    y_hat_one_hot\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SI1', 'VS2', 'SI2', 'VS1', 'VVS2', 'VVS1', 'IF', 'I1']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clarity'].value_counts().keys().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
