{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.style.use('whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "matplotlib.rcParams.update({'figure.figsize': (10, 6)})\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'axes.labelsize': 20})\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'font.family': 'Helvetica, Arial, sans-serif'})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>clarity</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045908</td>\n",
       "      <td>0.367784</td>\n",
       "      <td>0.067572</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>SI2</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.362197</td>\n",
       "      <td>0.065195</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.756962</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>SI1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045908</td>\n",
       "      <td>0.377095</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.720253</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>VS1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.057884</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.071817</td>\n",
       "      <td>0.082704</td>\n",
       "      <td>0.789873</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.017744</td>\n",
       "      <td>VS2</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.073854</td>\n",
       "      <td>0.086478</td>\n",
       "      <td>0.801266</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>SI2</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat         x         y         z     depth     table     price  \\\n",
       "0  0.045908  0.367784  0.067572  0.076415  0.778481  0.578947  0.017319   \n",
       "1  0.041916  0.362197  0.065195  0.072642  0.756962  0.642105  0.017319   \n",
       "2  0.045908  0.377095  0.069100  0.072642  0.720253  0.684211  0.017372   \n",
       "3  0.057884  0.391061  0.071817  0.082704  0.789873  0.610526  0.017744   \n",
       "4  0.061876  0.404097  0.073854  0.086478  0.801266  0.610526  0.017797   \n",
       "\n",
       "  clarity color  \n",
       "0     SI2     E  \n",
       "1     SI1     E  \n",
       "2     VS1     E  \n",
       "3     VS2     I  \n",
       "4     SI2     J  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/shivam2503/diamonds/data\n",
    "diamonds = pd.read_csv('data/diamonds.csv')\n",
    "\n",
    "from copy import deepcopy\n",
    "'''\n",
    "data = deepcopy(diamonds[['carat', 'x', 'y', 'z', 'depth', 'table',\n",
    "             'clarity', 'color']])\n",
    "'''\n",
    "data = deepcopy(diamonds[['carat', 'x', 'y', 'z', 'depth', 'table', 'price',\n",
    "                          'clarity', 'color']])\n",
    "target = deepcopy(diamonds['cut']).astype('category')\n",
    "\n",
    "for col in ['carat','x','y','z','depth','table','price']:\n",
    "    data[col] = data[col]/data[col].max()\n",
    "\n",
    "\n",
    "for col in ['color', 'clarity']:\n",
    "    data[col] = data[col].astype('category');\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "##### KERAS #####\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project6-PRT88SwD/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "numeric_headers = ['x', 'y', 'z', 'depth', 'table']\n",
    "\n",
    "X_train_num =  data_train[numeric_headers].values\n",
    "X_test_num = data_test[numeric_headers].values\n",
    "\n",
    "encoders = dict() \n",
    "categorical_headers = ['color','clarity']\n",
    "\n",
    "for col in categorical_headers:\n",
    "    data_train[col] = data_train[col].str.strip()\n",
    "    data_test[col] = data_test[col].str.strip()\n",
    "    encoders[col] = LabelEncoder()\n",
    "    data_train[col+'_int'] = encoders[col].fit_transform(data_train[col])\n",
    "    data_test[col+'_int'] = encoders[col].transform(data_test[col])\n",
    "    \n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"numeric_data:0\", shape=(?, 5), dtype=float32)\n",
      "51792       Good\n",
      "30014      Ideal\n",
      "19139    Premium\n",
      "38884      Ideal\n",
      "46697      Ideal\n",
      "Name: cut, dtype: category\n",
      "Categories (5, object): [Fair, Good, Ideal, Premium, Very Good]\n",
      "Epoch 1/50\n",
      "43152/43152 [==============================] - 6s - loss: 1.2760 - categorical_accuracy: 0.4661 - acc: 0.4661     \n",
      "Epoch 2/50\n",
      "43152/43152 [==============================] - 5s - loss: 1.0847 - categorical_accuracy: 0.5599 - acc: 0.5599     \n",
      "Epoch 3/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.9797 - categorical_accuracy: 0.6041 - acc: 0.6041     \n",
      "Epoch 4/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.9047 - categorical_accuracy: 0.6404 - acc: 0.6404     \n",
      "Epoch 5/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.8464 - categorical_accuracy: 0.6639 - acc: 0.6639     \n",
      "Epoch 6/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.8071 - categorical_accuracy: 0.6827 - acc: 0.6827     \n",
      "Epoch 7/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7937 - categorical_accuracy: 0.6859 - acc: 0.6859     \n",
      "Epoch 8/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7816 - categorical_accuracy: 0.6920 - acc: 0.6920     \n",
      "Epoch 9/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7745 - categorical_accuracy: 0.6945 - acc: 0.6945     \n",
      "Epoch 10/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7659 - categorical_accuracy: 0.6996 - acc: 0.6996     \n",
      "Epoch 11/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7650 - categorical_accuracy: 0.7006 - acc: 0.7006     \n",
      "Epoch 12/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7593 - categorical_accuracy: 0.7025 - acc: 0.7025     \n",
      "Epoch 13/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7560 - categorical_accuracy: 0.7038 - acc: 0.7038     \n",
      "Epoch 14/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7559 - categorical_accuracy: 0.7038 - acc: 0.7038     \n",
      "Epoch 15/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7554 - categorical_accuracy: 0.7025 - acc: 0.7025     \n",
      "Epoch 16/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7574 - categorical_accuracy: 0.7023 - acc: 0.7023     \n",
      "Epoch 17/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7476 - categorical_accuracy: 0.7060 - acc: 0.7060     \n",
      "Epoch 18/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7493 - categorical_accuracy: 0.7062 - acc: 0.7062     \n",
      "Epoch 19/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7460 - categorical_accuracy: 0.7074 - acc: 0.7074     \n",
      "Epoch 20/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7472 - categorical_accuracy: 0.7049 - acc: 0.7049     \n",
      "Epoch 21/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7402 - categorical_accuracy: 0.7098 - acc: 0.7098     \n",
      "Epoch 22/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7478 - categorical_accuracy: 0.7067 - acc: 0.7067     \n",
      "Epoch 23/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7436 - categorical_accuracy: 0.7084 - acc: 0.7084     \n",
      "Epoch 24/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7377 - categorical_accuracy: 0.7092 - acc: 0.7092     \n",
      "Epoch 25/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7351 - categorical_accuracy: 0.7113 - acc: 0.7113     \n",
      "Epoch 26/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7310 - categorical_accuracy: 0.7126 - acc: 0.7126     \n",
      "Epoch 27/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7294 - categorical_accuracy: 0.7131 - acc: 0.7131     \n",
      "Epoch 28/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7212 - categorical_accuracy: 0.7169 - acc: 0.7169     \n",
      "Epoch 29/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.7077 - categorical_accuracy: 0.7223 - acc: 0.7223     \n",
      "Epoch 30/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6940 - categorical_accuracy: 0.7271 - acc: 0.7271     \n",
      "Epoch 31/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6756 - categorical_accuracy: 0.7372 - acc: 0.7372     \n",
      "Epoch 32/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6599 - categorical_accuracy: 0.7417 - acc: 0.7417     \n",
      "Epoch 33/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6578 - categorical_accuracy: 0.7400 - acc: 0.7400     \n",
      "Epoch 34/50\n",
      "43152/43152 [==============================] - 6s - loss: 0.6460 - categorical_accuracy: 0.7461 - acc: 0.7461     \n",
      "Epoch 35/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6387 - categorical_accuracy: 0.7502 - acc: 0.7502     \n",
      "Epoch 36/50\n",
      "43152/43152 [==============================] - 6s - loss: 0.6322 - categorical_accuracy: 0.7506 - acc: 0.7506     \n",
      "Epoch 37/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6285 - categorical_accuracy: 0.7519 - acc: 0.7519     \n",
      "Epoch 38/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6252 - categorical_accuracy: 0.7532 - acc: 0.7532     \n",
      "Epoch 39/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6237 - categorical_accuracy: 0.7525 - acc: 0.7525     \n",
      "Epoch 40/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6203 - categorical_accuracy: 0.7572 - acc: 0.7572     \n",
      "Epoch 41/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6140 - categorical_accuracy: 0.7575 - acc: 0.7575     \n",
      "Epoch 42/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6179 - categorical_accuracy: 0.7565 - acc: 0.7565     \n",
      "Epoch 43/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6131 - categorical_accuracy: 0.7589 - acc: 0.7589     \n",
      "Epoch 44/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6168 - categorical_accuracy: 0.7577 - acc: 0.7577     \n",
      "Epoch 45/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6089 - categorical_accuracy: 0.7583 - acc: 0.7583     \n",
      "Epoch 46/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6041 - categorical_accuracy: 0.7614 - acc: 0.7614     \n",
      "Epoch 47/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6083 - categorical_accuracy: 0.7589 - acc: 0.7589     \n",
      "Epoch 48/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.5997 - categorical_accuracy: 0.7646 - acc: 0.7646     \n",
      "Epoch 49/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6027 - categorical_accuracy: 0.7614 - acc: 0.7614     \n",
      "Epoch 50/50\n",
      "43152/43152 [==============================] - 5s - loss: 0.6017 - categorical_accuracy: 0.7623 - acc: 0.7623     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f2de390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment key\n",
    "# #   = My comments\n",
    "# ##  = Larson comments\n",
    "# ### = Changed code\n",
    "\n",
    "\n",
    "# Columns to generate 1-hots\n",
    "#  education    X occupation\n",
    "#  country      X occupation\n",
    "#  relationship X marital_status X sex\n",
    "#  race         X sex\n",
    "cross_columns = [['clarity', 'color']]\n",
    "\n",
    "\n",
    "# Define the order and layers of the network????\n",
    "## we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "\n",
    "# For all sets of columns to be crossed\n",
    "for cols in cross_columns:\n",
    "    # Creates labels for categorical data. And stores the map\n",
    "    #  enc.transform translates categorical data to integer\n",
    "    ## encode crossed columns as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    ## create crossed labels\n",
    "    # Create dataframe of the columns cross product as string data\n",
    "    X_crossed_train = data_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = data_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # fits the label encoder to the [x_crossed_train, x_crossed_test]\n",
    "    enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "    # transform the string to integer for train data\n",
    "    X_crossed_train = enc.transform(X_crossed_train)\n",
    "    # transform the string to integer for test data\n",
    "    X_crossed_test = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # Add elements of x_crossed_train to list of previous x_crossed_train values\n",
    "    X_ints_train.append( X_crossed_train )\n",
    "    # Same with test data\n",
    "    X_ints_test.append( X_crossed_test )\n",
    "    \n",
    "    ## get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) ## same as the max(df_train[col])\n",
    "    ## create embedding branch from the number of categories\n",
    "    # Create inputs for each of the crossed columns\n",
    "    inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "    # Adds the Inputs to a list\n",
    "    all_inputs.append(inputs)\n",
    "    # Creates an Embedding with input number of categories and output the sqrt(#categories). \n",
    "    #  input_length is max matrix size of input\n",
    "    # Passes inputs into embedding\n",
    "\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    \n",
    "    # Flatten the dimension of the Embedding\n",
    "    x = Flatten()(x)\n",
    "    # Add the Flattened Embedding to the list\n",
    "    all_branch_outputs.append(x)\n",
    "\n",
    "# Merge all of the Flattened branches to create a wide_branch\n",
    "## merge the branches together\n",
    "### wide_branch = concatenate(all_branch_outputs)\n",
    "wide_branch = all_branch_outputs[0]\n",
    "\n",
    "## reset this input branch\n",
    "all_branch_outputs = []\n",
    "## add in the embeddings\n",
    "for col in categorical_headers_ints:\n",
    "    ## encode as ints for the embedding\n",
    "    X_ints_train.append( data_train[col].values ) # append NDarrays to list\n",
    "    X_ints_test.append( data_test[col].values )\n",
    "    \n",
    "    ## get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) ## same as the max(df_train[col])\n",
    "    # Input defines the tensor shape\n",
    "    ## create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    # Create a list of all the Inputs\n",
    "    all_inputs.append(inputs)\n",
    "    \n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "## also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False, name='numeric_data'))\n",
    "# Create 20 node dense NN with relu activation\n",
    "print(all_inputs[-1])\n",
    "'''\n",
    "x = Dense(units=5, activation='relu')(all_inputs[-1])\n",
    "'''\n",
    "x = Dense(units=200, activation='linear')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "## merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "'''\n",
    "deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "'''\n",
    "deep_branch = Dense(units=75,activation='relu')(deep_branch)\n",
    "deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    \n",
    "final_branch = concatenate([wide_branch, deep_branch])\n",
    "final_branch = Dense(units=5,activation='sigmoid')(final_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "'''model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['categorical_accuracy'])\n",
    "              \n",
    "    SGD = 41%\n",
    "    RMSprop(lr=0.001) = 57%\n",
    "    Adagrad = 55%\n",
    "    Adadelta = 55%\n",
    "    Nadam(epochs=20) = 70%\n",
    "'''\n",
    "# Best is categorical_crossentropy and mean_squared_error\n",
    "model.compile(optimizer='Nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy', 'accuracy'])\n",
    "\n",
    "\n",
    "print(target_train.head())\n",
    "\n",
    "target_train_one_hot = pd.get_dummies(target_train)\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        np.asarray(target_train_one_hot), epochs=50, batch_size=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_ints_test+[X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 254   52    2    7    4]\n",
      " [  12  626    9   14  289]\n",
      " [   2    9 4133  118  160]\n",
      " [   0   20  305 2129  275]\n",
      " [   0  149  609  284 1326]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11f58dc50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDgAAALHCAYAAABrBPJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAg4AAACg7Ak4AAAAgLL3f9YcJxFRuyP5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ee9a400>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 355,
       "width": 540
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "target_test_one_hot = pd.get_dummies(target_test)\n",
    "cnf = confusion_matrix(\n",
    "    np.asarray(target_test_one_hot).argmax(1),\n",
    "    y_hat.argmax(1)\n",
    ")\n",
    "print(cnf)\n",
    "cnf_norm = cnf.astype('float') / cnf.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cnf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class WideDeepNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self, activation='relu', final_activation='sigmoid', optimizer='adagrad', \n",
    "        loss='mean_squared_error', metrics=['accuracy']\n",
    "    ):\n",
    "        self.activation = activation\n",
    "        self.final_activation = final_activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def _wide_network():\n",
    "        # Get the categorical headers\n",
    "        cross_columns = list(self.X.select_dtype('category').columns)\n",
    "\n",
    "        ## we need to create separate sequential models for each embedding\n",
    "        embed_branches = []\n",
    "        X_ints_train = []\n",
    "        X_ints_test = []\n",
    "        all_inputs = []\n",
    "        all_branch_outputs = []\n",
    "\n",
    "\n",
    "        # For all sets of columns to be crossed\n",
    "        for cols in cross_columns:\n",
    "            # Creates labels for categorical data. And stores the map\n",
    "            #  enc.transform translates categorical data to integer\n",
    "            ## encode crossed columns as ints for the embedding\n",
    "            enc = LabelEncoder()\n",
    "\n",
    "            ## create crossed labels\n",
    "            # Create dataframe of the columns cross product as string data\n",
    "            X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "            X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "            # fits the label encoder to the [x_crossed_train, x_crossed_test]\n",
    "            enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "            # transform the string to integer for train data\n",
    "            X_crossed_train = enc.transform(X_crossed_train)\n",
    "            # transform the string to integer for test data\n",
    "            X_crossed_test = enc.transform(X_crossed_test)\n",
    "\n",
    "            # Add elements of x_crossed_train to list of previous x_crossed_train values\n",
    "            X_ints_train.append( X_crossed_train )\n",
    "            # Same with test data\n",
    "            X_ints_test.append( X_crossed_test )\n",
    "\n",
    "            print(X_ints_train[-1])\n",
    "            ## get the number of categories\n",
    "            N = max(X_ints_train[-1]+1) ## same as the max(df_train[col])\n",
    "            ## create embedding branch from the number of categories\n",
    "            # Create inputs for each of the crossed columns\n",
    "            inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "            # Adds the Inputs to a list\n",
    "            all_inputs.append(inputs)\n",
    "            # Creates an Embedding with input number of categories and output the sqrt(#categories). \n",
    "            #  input_length is max matrix size of input\n",
    "            # Passes inputs into embedding\n",
    "\n",
    "            print('Input dim for embedding', N)\n",
    "            x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "            # Flatten the dimension of the Embedding\n",
    "            x = Flatten()(x)\n",
    "            # Add the Flattened Embedding to the list\n",
    "            all_branch_outputs.append(x)\n",
    "\n",
    "        # Merge all of the Flattened branches to create a wide_branch\n",
    "        ## merge the branches together\n",
    "        ### wide_branch = concatenate(all_branch_outputs)\n",
    "        wide_branch = all_branch_outputs[0]\n",
    "        return wide_branch\n",
    "        \n",
    "    def _make_model(self):\n",
    "        wide_branch = self._wide_network()\n",
    "        deep_branch = self._deep_network()\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self._make_mode()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_hat = predict(X)\n",
    "        return y_hat\n",
    "        \n",
    "    # [CITE] http://algoadventures.com/sklearn-from-the-source-code-up-basics/\n",
    "    # ClassifierMixin implementation\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
    "    # end implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-eba159eba1fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# gets the column of the maximum for each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# gets the column of the maximum for each row\n",
    "# then converts it to binary (manual one-hot encoding)\n",
    "y_hat_one_hot = np.zeros(y_hat.shape)\n",
    "y_hat_one_hot[np.arange(y_hat.shape[0]), y_hat.argmax(1)] = 1\n",
    "\n",
    "# ROC Area under Curve score\n",
    "roc_auc_score(\n",
    "    np.asarray(y_test),\n",
    "    y_hat_one_hot\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "g = list(data.select_dtypes(include='category').columns)\n",
    "cross = list(itertools.combinations(g, 2))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ideal        21551\n",
       "Premium      13791\n",
       "Very Good    12082\n",
       "Good          4906\n",
       "Fair          1610\n",
       "Name: cut, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
